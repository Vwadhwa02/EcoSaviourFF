{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense\n",
    "from keras.layers import LeakyReLU\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import random\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "from os.path import dirname,join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_H, GRID_W = 13 , 13\n",
    "BOX = 5\n",
    "ORIG_CLASS = 20\n",
    "\n",
    "WT_PATH = 'modell.h5'\n",
    "\n",
    "THRESHOLD = 0.2\n",
    "ANCHORS = '1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52'\n",
    "ANCHORS = [float(ANCHORS.strip()) for ANCHORS in ANCHORS.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, class_num):\n",
    "        self.x, self.y, self.w, self.h, self.c = 0., 0., 0., 0., 0.\n",
    "        self.probs = np.zeros((class_num,))\n",
    "\n",
    "    def iou(self, box):\n",
    "        intersection = self.intersect(box)\n",
    "        union = self.w*self.h + box.w*box.h - intersection\n",
    "        return intersection/union\n",
    "\n",
    "    def intersect(self, box):\n",
    "        width  = self.__overlap([self.x-self.w/2, self.x+self.w/2], [box.x-box.w/2, box.x+box.w/2])\n",
    "        height = self.__overlap([self.y-self.h/2, self.y+self.h/2], [box.y-box.h/2, box.y+box.h/2])\n",
    "        return width * height\n",
    "\n",
    "    def __overlap(self, interval_a, interval_b):\n",
    "        x1, x2 = interval_a\n",
    "        x3, x4 = interval_b\n",
    "        if x3 < x1:\n",
    "            if x4 < x1:\n",
    "                return 0\n",
    "            else:\n",
    "                return min(x2,x4) - x1\n",
    "        else:\n",
    "            if x2 < x3:\n",
    "                return 0\n",
    "            else:\n",
    "                return min(x2,x4) - x3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_netout(image, netout):\n",
    "    boxes = []\n",
    "\n",
    "    # interpret the output by the network\n",
    "    for row in range(GRID_H):\n",
    "        for col in range(GRID_W):\n",
    "            for b in range(BOX):\n",
    "                box = BoundBox(CLASS)\n",
    "\n",
    "                # first 5 weights for x, y, w, h and confidence\n",
    "                box.x, box.y, box.w, box.h, box.c = netout[row,col,b,:5]\n",
    "\n",
    "                box.x = (col + sigmoid(box.x)) / GRID_W\n",
    "                box.y = (row + sigmoid(box.y)) / GRID_H\n",
    "                box.w = ANCHORS[2 * b + 0] * np.exp(box.w) / GRID_W\n",
    "                box.h = ANCHORS[2 * b + 1] * np.exp(box.h) / GRID_H\n",
    "                box.c = sigmoid(box.c)\n",
    "\n",
    "                # rest of weights for class likelihoods\n",
    "                classes = netout[row,col,b,5:]\n",
    "                box.probs = softmax(classes) * box.c\n",
    "                box.probs *= box.probs > THRESHOLD\n",
    "\n",
    "                boxes.append(box)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    for c in range(CLASS):\n",
    "        sorted_indices = list(reversed(np.argsort([box.probs[c] for box in boxes])))\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].probs[c] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(sorted_indices)):\n",
    "                    index_j = sorted_indices[j]\n",
    "\n",
    "                    if boxes[index_i].iou(boxes[index_j]) >= 0.4:\n",
    "                        boxes[index_j].probs[c] = 0\n",
    "\n",
    "    \n",
    "\n",
    "    # draw the boxes using a threshold\n",
    "    for box in boxes:\n",
    "        max_indx = np.argmax(box.probs)\n",
    "        max_prob = box.probs[max_indx]\n",
    "        \n",
    "        # if(max_prob>0.01):\n",
    "        # \tprint(\"Detected box with probability : {}\".format(max_prob))\n",
    "\n",
    "\n",
    "        if max_prob > THRESHOLD:\n",
    "            xmin  = int((box.x - box.w/2) * image.shape[1])\n",
    "            xmax  = int((box.x + box.w/2) * image.shape[1])\n",
    "            ymin  = int((box.y - box.h/2) * image.shape[0])\n",
    "            ymax  = int((box.y + box.h/2) * image.shape[0])\n",
    "\n",
    "            cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (200,0,0), 2)\n",
    "            cv2.putText(image, labels[max_indx], (xmin, ymin - 12), 0, 1e-3 * image.shape[0], (0,255,0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1.  + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WeightReader:\n",
    "#     def __init__(self, weight_file):\n",
    "#         self.offset = 4\n",
    "#         self.path_w='tiny-yolo-voc.weights'\n",
    "#         self.all_weights = np.fromfile(self.path_w, dtype='float32')\n",
    "        \n",
    "#     def read_bytes(self, size):\n",
    "#         self.offset = self.offset + size\n",
    "#         return self.all_weights[self.offset-size:self.offset]\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.offset = 4\n",
    "\n",
    "# weight_reader = WeightReader(WT_PATH)\n",
    "# #weight_reader=WeightReader(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modell.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\tiny_yolo_chaquopy_updated-main\\tiny_yolo_chaquopy_updated-main\\app\\src\\main\\python\\model app sample.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/tiny_yolo_chaquopy_updated-main/tiny_yolo_chaquopy_updated-main/app/src/main/python/model%20app%20sample.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/tiny_yolo_chaquopy_updated-main/tiny_yolo_chaquopy_updated-main/app/src/main/python/model%20app%20sample.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/tiny_yolo_chaquopy_updated-main/tiny_yolo_chaquopy_updated-main/app/src/main/python/model%20app%20sample.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m weight_reader\u001b[39m=\u001b[39mModelRead(WT_PATH)\n",
      "\u001b[1;32mf:\\tiny_yolo_chaquopy_updated-main\\tiny_yolo_chaquopy_updated-main\\app\\src\\main\\python\\model app sample.ipynb Cell 8\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/tiny_yolo_chaquopy_updated-main/tiny_yolo_chaquopy_updated-main/app/src/main/python/model%20app%20sample.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/tiny_yolo_chaquopy_updated-main/tiny_yolo_chaquopy_updated-main/app/src/main/python/model%20app%20sample.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_w\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodell.h5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/tiny_yolo_chaquopy_updated-main/tiny_yolo_chaquopy_updated-main/app/src/main/python/model%20app%20sample.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfromfile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_w, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modell.h5'"
     ]
    }
   ],
   "source": [
    "class ModelRead:\n",
    "    def __init__(self,mod_file):\n",
    "        self.offset = 4\n",
    "        self.path_w='modell.h5'\n",
    "        self.all_weights = np.fromfile(self.path_w, dtype='float32')\n",
    "        #self.model = load_model('modell.h5')\n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "    def reset(self):\n",
    "        self.offset = 4\n",
    "\n",
    "weight_reader=ModelRead(WT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(416,416,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2 - 5\n",
    "for i in range(0,4):\n",
    "    model.add(Conv2D(32*(2**i), (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='same'))\n",
    "\n",
    "# Layer 7 - 8\n",
    "for _ in range(0,2):\n",
    "    model.add(Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "# Layer 9\n",
    "model.add(Conv2D(BOX * (4 + 1 + ORIG_CLASS), (1, 1), strides=(1, 1), kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Reshape((GRID_H, GRID_W, BOX, 4 + 1 + ORIG_CLASS)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wei=\"F:\\tiny_yolo_chaquopy_updated-main\\tiny_yolo_chaquopy_updated-main\\app\\src\\main\\python\\tiny-yolo-voc.weights\"\n",
    "# wei.reset()\n",
    "weight_reader.reset()\n",
    "nb_conv = 8\n",
    "\n",
    "for i in range(1, nb_conv+1):\n",
    "    conv_layer = model.get_layer('conv2d_' + str(i))\n",
    "    \n",
    "    if i < nb_conv:\n",
    "        norm_layer = model.get_layer('batch_normalization_' + str(i))\n",
    "        \n",
    "        size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "\n",
    "        beta  = weight_reader.read_bytes(size)\n",
    "        gamma = weight_reader.read_bytes(size)\n",
    "        mean  = weight_reader.read_bytes(size)\n",
    "        var   = weight_reader.read_bytes(size)\n",
    "\n",
    "        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n",
    "        \n",
    "    if len(conv_layer.get_weights()) > 1:\n",
    "        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel, bias])\n",
    "    else:\n",
    "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2,3,1,0])\n",
    "        conv_layer.set_weights([kernel])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recycle', 'organic'}\n"
     ]
    }
   ],
   "source": [
    "labels = ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
    "label={\"Recycle\",\"organic\"}\n",
    "print (label);\n",
    "\n",
    "# CLASS = 184\n",
    "CLASS = 2 # 23\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectObjects(image):\n",
    "    input_image = cv2.resize(image, (416, 416))\n",
    "    input_image = input_image / 255.\n",
    "    input_image = input_image[:,:,::-1]\n",
    "    input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "    # netout = new_model.predict(input_image)\n",
    "    netout = model.predict(input_image)\n",
    "    image = interpret_netout(image, netout[0])\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_draw(strn):\n",
    "    decoded_data=base64.b64decode(strn)\n",
    "    np_data=np.fromstring(decoded_data,np.uint8)\n",
    "    img=cv2.imdecode(np_data,cv2.IMREAD_UNCHANGED)\n",
    "   \n",
    "    img=detectObjects(img)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    pil_img=Image.fromarray(img)\n",
    "    buff=io.BytesIO()\n",
    "    pil_img.save(buff,format=\"PNG\")\n",
    "    img_str=base64.b64encode(buff.getvalue())\n",
    "    return \"\"+str(img_str,'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
